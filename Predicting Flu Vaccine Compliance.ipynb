{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Flu Vaccine Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The CDC wants to have a model that can predict who does and who does not get the seasonal flu vaccine. \n",
    "\n",
    "They have provided the National 2009 H1N1 Flu Survey Data for the purposes of building a model that can predict who will and won't get the flu vaccine. They hope to use the best features from this model in order to later build other predictive models that can predict compliance with COVID-19 vaccines, as currently the boosters are routine, like the seasonal flu vaccines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buisness Problem\n",
    "\n",
    "Creating a model in order to pull out the features that best predict who will get the seasonal flu vaccine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size \n",
    "\n",
    "The initial dataset was 26,707 rows with 36 columns. After data cleaning there were 27 columns. \n",
    "\n",
    "### Limitations of the Dataset\n",
    "\n",
    "The data was collected via telephone surveys, a commonly used polling method which is not representative or random (as people choose to respond or not when they are called). Additionally, today new models may need to take into account the anti-vaccine movement (article [here](https://pubmed.ncbi.nlm.nih.gov/16039769/)) which was not as prevalent when the data was collected in 2009 - this may affect the accuracy of the models built if applied to more recent datasets. \n",
    "\n",
    "### Dataset Size \n",
    "\n",
    "The initial dataset was 26,707 rows with 36 columns. After data cleaning there were 27 relevant columns. \n",
    "\n",
    "### Limitations Of The Dataset\n",
    "\n",
    "The data was collected via telephone surveys, a commonly used polling method which is not representative or random (as people choose to respond or not when they are called). Additionally, today new models may need to take into account the anti-vaccine movement (article [here](https://pubmed.ncbi.nlm.nih.gov/16039769/)) which was not as prevalent when the data was collected in 2009 - this may affect the accuracy of the models built now on more current datasets. \n",
    "\n",
    "\n",
    "### Why We Used This Dataset\n",
    "\n",
    "Despite the above limitations, the dataset does contain a large number of responses on and takes into account a large number of features relevant to the seasonal flu vaccine compliance, and is a relatively recent dataset. For all these reasons, we decided to use this dataset to create our predictive model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Data\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import scipy.sparse\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"Data/training_set_features.csv\")\n",
    "labels = pd.read_csv(\"Data/training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the features and lable dataframes match up \n",
    "np.testing.assert_array_equal(features.index.values, labels.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>income_poverty</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...             income_poverty  marital_status  \\\n",
       "0                    1.0  ...              Below Poverty     Not Married   \n",
       "1                    1.0  ...              Below Poverty     Not Married   \n",
       "2                    0.0  ...  <= $75,000, Above Poverty     Not Married   \n",
       "3                    0.0  ...              Below Poverty     Not Married   \n",
       "4                    1.0  ...  <= $75,000, Above Poverty         Married   \n",
       "\n",
       "   rent_or_own   employment_status  hhs_geo_region                census_msa  \\\n",
       "0          Own  Not in Labor Force        oxchjgsf                   Non-MSA   \n",
       "1         Rent            Employed        bhuqouqj  MSA, Not Principle  City   \n",
       "2          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "3         Rent  Not in Labor Force        lrircsnp       MSA, Principle City   \n",
       "4          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "\n",
       "   household_adults  household_children  employment_industry  \\\n",
       "0               0.0                 0.0                  NaN   \n",
       "1               0.0                 0.0             pxcmvdjn   \n",
       "2               2.0                 0.0             rucpziij   \n",
       "3               0.0                 0.0                  NaN   \n",
       "4               1.0                 0.0             wxleyezf   \n",
       "\n",
       "   employment_occupation  \n",
       "0                    NaN  \n",
       "1               xgwztkwe  \n",
       "2               xtkaffoo  \n",
       "3                    NaN  \n",
       "4               emcorrxb  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at the features data\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "      <th>seasonal_vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_vaccine  seasonal_vaccine\n",
       "0              0             0                 0\n",
       "1              1             0                 1\n",
       "2              2             0                 0\n",
       "3              3             0                 1\n",
       "4              4             0                 0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets look into the labels data\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26707 entries, 0 to 26706\n",
      "Data columns (total 36 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   respondent_id                26707 non-null  int64  \n",
      " 1   h1n1_concern                 26615 non-null  float64\n",
      " 2   h1n1_knowledge               26591 non-null  float64\n",
      " 3   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 4   behavioral_avoidance         26499 non-null  float64\n",
      " 5   behavioral_face_mask         26688 non-null  float64\n",
      " 6   behavioral_wash_hands        26665 non-null  float64\n",
      " 7   behavioral_large_gatherings  26620 non-null  float64\n",
      " 8   behavioral_outside_home      26625 non-null  float64\n",
      " 9   behavioral_touch_face        26579 non-null  float64\n",
      " 10  doctor_recc_h1n1             24547 non-null  float64\n",
      " 11  doctor_recc_seasonal         24547 non-null  float64\n",
      " 12  chronic_med_condition        25736 non-null  float64\n",
      " 13  child_under_6_months         25887 non-null  float64\n",
      " 14  health_worker                25903 non-null  float64\n",
      " 15  health_insurance             14433 non-null  float64\n",
      " 16  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 17  opinion_h1n1_risk            26319 non-null  float64\n",
      " 18  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 19  opinion_seas_vacc_effective  26245 non-null  float64\n",
      " 20  opinion_seas_risk            26193 non-null  float64\n",
      " 21  opinion_seas_sick_from_vacc  26170 non-null  float64\n",
      " 22  age_group                    26707 non-null  object \n",
      " 23  education                    25300 non-null  object \n",
      " 24  race                         26707 non-null  object \n",
      " 25  sex                          26707 non-null  object \n",
      " 26  income_poverty               22284 non-null  object \n",
      " 27  marital_status               25299 non-null  object \n",
      " 28  rent_or_own                  24665 non-null  object \n",
      " 29  employment_status            25244 non-null  object \n",
      " 30  hhs_geo_region               26707 non-null  object \n",
      " 31  census_msa                   26707 non-null  object \n",
      " 32  household_adults             26458 non-null  float64\n",
      " 33  household_children           26458 non-null  float64\n",
      " 34  employment_industry          13377 non-null  object \n",
      " 35  employment_occupation        13237 non-null  object \n",
      "dtypes: float64(23), int64(1), object(12)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Looking into the `features` DataFrame\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptions of the features (taken from [here](https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/)): \n",
    "\n",
    "For all binary variables: 0 = No; 1 = Yes.\n",
    "\n",
    "- h1n1_concern - Level of concern about the H1N1 flu. 0 = Not at all concerned; 1 = Not very concerned; 2 = Somewhat concerned; 3 = Very concerned.\n",
    "- h1n1_knowledge - Level of knowledge about H1N1 flu. 0 = No knowledge; 1 = A little knowledge; 2 = A lot of knowledge.\n",
    "- behavioral_antiviral_meds - Has taken antiviral medications. (binary)\n",
    "- behavioral_avoidance - Has avoided close contact with others with flu-like symptoms. (binary)\n",
    "- behavioral_face_mask - Has bought a face mask. (binary)\n",
    "- behavioral_wash_hands - Has frequently washed hands or used hand sanitizer. (binary)\n",
    "- behavioral_large_gatherings - Has reduced time at large gatherings. (binary)\n",
    "- behavioral_outside_home - Has reduced contact with people outside of own household. (binary)\n",
    "- behavioral_touch_face - Has avoided touching eyes, nose, or mouth. (binary)\n",
    "- doctor_recc_h1n1 - H1N1 flu vaccine was recommended by doctor. (binary)\n",
    "- doctor_recc_seasonal - Seasonal flu vaccine was recommended by doctor. (binary)\n",
    "- chronic_med_condition - Has any of the following chronic medical conditions: asthma or an other lung condition, diabetes, a heart condition, a kidney condition, sickle cell anemia or other anemia, a neurological or neuromuscular condition, a liver condition, or a weakened immune system caused by a chronic illness or by medicines taken for a chronic illness. (binary)\n",
    "- child_under_6_months - Has regular close contact with a child under the age of six months. (binary)\n",
    "- health_worker - Is a healthcare worker. (binary)\n",
    "- health_insurance - Has health insurance. (binary)\n",
    "- opinion_h1n1_vacc_effective - Respondent's opinion about H1N1 vaccine effectiveness. 1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective.\n",
    "- opinion_h1n1_risk - Respondent's opinion about risk of getting sick with H1N1 flu without vaccine.1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high.\n",
    "- opinion_h1n1_sick_from_vacc - Respondent's worry of getting sick from taking H1N1 vaccine.1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried.\n",
    "- opinion_seas_vacc_effective - Respondent's opinion about seasonal flu vaccine effectiveness.1 = Not at all effective; 2 = Not very effective; 3 = Don't know; 4 = Somewhat effective; 5 = Very effective.\n",
    "- opinion_seas_risk - Respondent's opinion about risk of getting sick with seasonal flu without vaccine.1 = Very Low; 2 = Somewhat low; 3 = Don't know; 4 = Somewhat high; 5 = Very high.\n",
    "- opinion_seas_sick_from_vacc - Respondent's worry of getting sick from taking seasonal flu vaccine.1 = Not at all worried; 2 = Not very worried; 3 = Don't know; 4 = Somewhat worried; 5 = Very worried.\n",
    "- age_group - Age group of respondent.\n",
    "- education - Self-reported education level.\n",
    "- race - Race of respondent.\n",
    "- sex - Sex of respondent.\n",
    "- income_poverty - Household annual income of respondent with respect to 2008 Census poverty thresholds.\n",
    "- marital_status - Marital status of respondent.\n",
    "- rent_or_own - Housing situation of respondent.\n",
    "- employment_status - Employment status of respondent.\n",
    "- hhs_geo_region - Respondent's residence using a 10-region geographic classification defined by the U.S. Dept. of Health and Human Services. Values are represented as short random character strings.\n",
    "- census_msa - Respondent's residence within metropolitan statistical areas (MSA) as defined by the U.S. Census.\n",
    "- household_adults - Number of other adults in household, top-coded to 3.\n",
    "- household_children - Number of children in household, top-coded to 3.\n",
    "- employment_industry - Type of industry respondent is employed in. Values are represented as short random character strings.\n",
    "- employment_occupation - Type of occupation of respondent. Values are represented as short random character strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Initial Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all of the columns related to only the H1N1 vaccine: h1n1_concern, h1n1_knowledge, doctor_recc_h1n1, opinion_h1n1_vacc_effective, opinion_h1n1_risk, opinion_h1n1_sick_from_vacc\n",
    "\n",
    "Additionally, there are some columns where the information has been scrambled (presumably to protect the respondents personal information) so lets remove those as well as we can't extract any useful information without knowing what they are coded for: hhs_geo_region, employment_industry, employment_occupation\n",
    "\n",
    "Finally, let's remove 'h1n1_vaccine' and 'respondent_id' from the labels DataFrame, and then turn the labels dataframe into an array so we can use it when we build our models later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1',\n",
    "               'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "               'opinion_h1n1_sick_from_vacc', 'hhs_geo_region', \n",
    "               'employment_industry', 'employment_occupation'], axis = 1, inplace = True)\n",
    "\n",
    "labels.drop(['h1n1_vaccine', 'respondent_id'], axis = 1, inplace= True)\n",
    "labels = np.ravel(labels, order = 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Preprocessing Pipeline\n",
    "\n",
    "In order to create a pipeline for preprocessing, let's first figure out how we will handle our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respondent_id                      0\n",
       "behavioral_antiviral_meds         71\n",
       "behavioral_avoidance             208\n",
       "behavioral_face_mask              19\n",
       "behavioral_wash_hands             42\n",
       "behavioral_large_gatherings       87\n",
       "behavioral_outside_home           82\n",
       "behavioral_touch_face            128\n",
       "doctor_recc_seasonal            2160\n",
       "chronic_med_condition            971\n",
       "child_under_6_months             820\n",
       "health_worker                    804\n",
       "health_insurance               12274\n",
       "opinion_seas_vacc_effective      462\n",
       "opinion_seas_risk                514\n",
       "opinion_seas_sick_from_vacc      537\n",
       "age_group                          0\n",
       "education                       1407\n",
       "race                               0\n",
       "sex                                0\n",
       "income_poverty                  4423\n",
       "marital_status                  1408\n",
       "rent_or_own                     2042\n",
       "employment_status               1463\n",
       "census_msa                         0\n",
       "household_adults                 249\n",
       "household_children               249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest amount of missing data is in `health_insurance` - let's look into the signifigance of that variable to see if we can drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0                0     1    All\n",
      "health_insurance                   \n",
      "0.0               1338   398   1736\n",
      "1.0               5866  6831  12697\n",
      "All               7204  7229  14433\n",
      "582.2867466798792 1.0559310374176207e-124\n"
     ]
    }
   ],
   "source": [
    "cross_tab = pd.crosstab(features['health_insurance'], labels, margins = True)\n",
    "print(cross_tab)\n",
    "chi2, p, dof, expected = chi2_contingency(cross_tab)\n",
    "print(chi2, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above a statistically significant p-value and a very high chi2 value. As such, I've decided to keep this feature. \n",
    "\n",
    "I've chosen to change all NaN's to 0, as in 2009 when the data was collected the Affordable Care Act had not been passed, so people were not required to have health insurance. I think that keeping the NaN's as 0's is better reflective of this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the rest of the missing variables:\n",
    "\n",
    " - For the rest of the **binary data** let's also replace all of the missing data with 0's. \n",
    " \n",
    " - For the opinion questions and family features (`household_adults` and `household_children`) which are numeric **ordinal and interval data** let's replace all the missing data with the median. Additionally, we will use `MinMaxScaler` to scale this data. Most of our data is binary, and `MinMaxScaler` will keep the scaled data in the range of 0-1, which is ideal in this case. \n",
    " \n",
    " - Finally, for our **categorical data** let's use `OneHotEncoder` to create dummy categories\n",
    " \n",
    " We will put all of this into a pipeline, so we can test out different models!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create functions for preprocessing\n",
    "\n",
    "# function to replace NaN's in the ordinal and interval data \n",
    "def replace_NAN_median(X_df):\n",
    "    opinions = ['opinion_seas_vacc_effective', 'opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'household_adults',\n",
    "                'household_children']\n",
    "    for column in opinions:\n",
    "        X_df[column].replace(np.nan, X_df[column].median(), inplace = True)\n",
    "    return X_df\n",
    "\n",
    "# function to replace NaN's in the catagorical data     \n",
    "def replace_NAN_mode(X_df):\n",
    "    miss_cat_features = ['education', 'income_poverty', 'marital_status', 'rent_or_own', 'employment_status']\n",
    "    for column in miss_cat_features:\n",
    "        X_df[column].replace(np.nan, statistics.mode(X_df[column]), inplace = True)\n",
    "    return X_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate transformers\n",
    "NAN_median = FunctionTransformer(replace_NAN_median)\n",
    "NAN_mode = FunctionTransformer(replace_NAN_mode)\n",
    "col_transformer = ColumnTransformer(transformers=\n",
    "    # replace NaN's in the binary data                                \n",
    "    [(\"NAN_0\", SimpleImputer(missing_values=np.nan, strategy='constant', fill_value = 0), \n",
    "    ['behavioral_antiviral_meds', 'behavioral_avoidance','behavioral_face_mask' ,\n",
    "    'behavioral_wash_hands', 'behavioral_large_gatherings', 'behavioral_outside_home',\n",
    "    'behavioral_touch_face', 'doctor_recc_seasonal', 'chronic_med_condition', \n",
    "    'child_under_6_months', 'health_worker', 'health_insurance']),\n",
    "    \n",
    "     # MinMaxScaler on our numeric ordinal and interval data\n",
    "    (\"scaler\", MinMaxScaler(), ['opinion_seas_vacc_effective', 'opinion_seas_risk',\n",
    "                                'opinion_seas_sick_from_vacc', \n",
    "                                'household_adults', 'household_children']),\n",
    "     \n",
    "     # OHE catagorical string data\n",
    "    (\"ohe\", OneHotEncoder(sparse = False), ['age_group','education', 'race', 'sex', \n",
    "                                'income_poverty', 'marital_status', 'rent_or_own',\n",
    "                                'employment_status', 'census_msa'])],\n",
    "     \n",
    "    remainder=\"passthrough\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline (Yey!)\n",
    "preprocessing_pipe = Pipeline(steps=[\n",
    "    (\"NAN_median\", NAN_median), \n",
    "    (\"NAN_mode\", NAN_mode), \n",
    "    (\"col_transformer\", col_transformer)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Create Baseline Model (LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our outcome data is binary, let's try using `LogisticRegression` to model our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our pipeline to instantiate the first model\n",
    "logreg_base_model_pipe = Pipeline(steps=[(\"preprocessing_pipe\", preprocessing_pipe),\n",
    "                                    (\"log_reg\", LogisticRegression(random_state = 42))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing_pipe',\n",
       "                 Pipeline(steps=[('NAN_median',\n",
       "                                  FunctionTransformer(func=<function replace_NAN_median at 0x000001A89012E550>)),\n",
       "                                 ('NAN_mode',\n",
       "                                  FunctionTransformer(func=<function replace_NAN_mode at 0x000001A89012E670>)),\n",
       "                                 ('col_transformer',\n",
       "                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                    transformers=[('NAN_0',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=...\n",
       "                                                                  ('scaler',\n",
       "                                                                   MinMaxScaler(),\n",
       "                                                                   ['opinion_seas_vacc_effective',\n",
       "                                                                    'opinion_seas_risk',\n",
       "                                                                    'opinion_seas_sick_from_vacc',\n",
       "                                                                    'household_adults',\n",
       "                                                                    'household_children']),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(sparse=False),\n",
       "                                                                   ['age_group',\n",
       "                                                                    'education',\n",
       "                                                                    'race',\n",
       "                                                                    'sex',\n",
       "                                                                    'income_poverty',\n",
       "                                                                    'marital_status',\n",
       "                                                                    'rent_or_own',\n",
       "                                                                    'employment_status',\n",
       "                                                                    'census_msa'])]))])),\n",
       "                ('log_reg', LogisticRegression(random_state=42))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model to our training data\n",
    "logreg_base_model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5310534198701947"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the mean accuracy of the model\n",
    "logreg_base_model_pipe.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized LogisticRegression() Model\n",
    "In our baseline `LogisticRegression` model, we got .53% accuracy - hardly better than a coin flip. \n",
    "\n",
    "Lets see if changing the parameters `solver`, `penalty`(if we use L1 (Lasso) or L2 (Ridge), and `C` (how strong the regularization strength is with smaller values being *stronger* regularization) will improve our accuracy. \n",
    "\n",
    "We can check all these things at once using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'log_reg__solver': ['liblinear'],\n",
    "    'log_reg__penalty': ['l1', 'l2'], \n",
    "    'log_reg__C': [0.001,0.01,0.1,1,10,100,1000]   \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=logreg_base_model_pipe,\n",
    "                  param_grid=param_grid,\n",
    "                  cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg_model_pipe.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessing_pipe',\n",
       "                                        Pipeline(steps=[('NAN_median',\n",
       "                                                         FunctionTransformer(func=<function replace_NAN_median at 0x000001A89012E550>)),\n",
       "                                                        ('NAN_mode',\n",
       "                                                         FunctionTransformer(func=<function replace_NAN_mode at 0x000001A89012E670>)),\n",
       "                                                        ('col_transformer',\n",
       "                                                         ColumnTransformer(remainder='passthrough',\n",
       "                                                                           transformers=[('NAN_0',\n",
       "                                                                                          SimpleIm...\n",
       "                                                                                           'household_children']),\n",
       "                                                                                         ('ohe',\n",
       "                                                                                          OneHotEncoder(sparse=False),\n",
       "                                                                                          ['age_group',\n",
       "                                                                                           'education',\n",
       "                                                                                           'race',\n",
       "                                                                                           'sex',\n",
       "                                                                                           'income_poverty',\n",
       "                                                                                           'marital_status',\n",
       "                                                                                           'rent_or_own',\n",
       "                                                                                           'employment_status',\n",
       "                                                                                           'census_msa'])]))])),\n",
       "                                       ('log_reg',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             param_grid={'log_reg__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'log_reg__penalty': ['l1', 'l2'],\n",
       "                         'log_reg__solver': ['liblinear']})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_reg__C': 10, 'log_reg__penalty': 'l1', 'log_reg__solver': 'liblinear'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the parameters with the best results \n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new model with the optimized parameters\n",
    "logreg_optimized_pipe =  Pipeline(steps=[(\"preprocessing_pipe\", preprocessing_pipe),\n",
    "                                    (\"log_reg\", LogisticRegression(solver = 'liblinear', random_state = 42, C = 10, penalty= 'l1'))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing_pipe',\n",
       "                 Pipeline(steps=[('NAN_median',\n",
       "                                  FunctionTransformer(func=<function replace_NAN_median at 0x000001A89012E550>)),\n",
       "                                 ('NAN_mode',\n",
       "                                  FunctionTransformer(func=<function replace_NAN_mode at 0x000001A89012E670>)),\n",
       "                                 ('col_transformer',\n",
       "                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                    transformers=[('NAN_0',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy=...\n",
       "                                                                   ['opinion_seas_vacc_effective',\n",
       "                                                                    'opinion_seas_risk',\n",
       "                                                                    'opinion_seas_sick_from_vacc',\n",
       "                                                                    'household_adults',\n",
       "                                                                    'household_children']),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(sparse=False),\n",
       "                                                                   ['age_group',\n",
       "                                                                    'education',\n",
       "                                                                    'race',\n",
       "                                                                    'sex',\n",
       "                                                                    'income_poverty',\n",
       "                                                                    'marital_status',\n",
       "                                                                    'rent_or_own',\n",
       "                                                                    'employment_status',\n",
       "                                                                    'census_msa'])]))])),\n",
       "                ('log_reg',\n",
       "                 LogisticRegression(C=10, penalty='l1', random_state=42,\n",
       "                                    solver='liblinear'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_optimized_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741387918122816"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_optimized_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.76108305e-02, -4.71093522e-02,  1.71787476e-03,\n",
       "         1.45673325e-01, -2.53400628e-03, -4.56066968e-02,\n",
       "         2.49698996e-01,  1.29665130e+00,  1.79070203e-01,\n",
       "         3.60191279e-02,  8.09461790e-01,  4.31263467e-01,\n",
       "         2.30594856e+00,  2.15343491e+00, -8.97631090e-01,\n",
       "        -2.00116903e-01, -1.51416275e-01, -8.72202563e-01,\n",
       "        -6.80397946e-01, -4.89292814e-01, -2.17216603e-01,\n",
       "         5.78978247e-01, -3.09803799e-01, -4.80350862e-01,\n",
       "        -5.86653093e-02, -2.45671390e-01, -6.64758153e-01,\n",
       "        -4.72492187e-01, -2.91403381e-01, -3.27679681e-01,\n",
       "        -4.22607616e-01, -4.09108570e-01, -3.56995968e-01,\n",
       "        -2.30700681e-01, -4.96987046e-01, -3.27058555e-01,\n",
       "        -4.68740678e-01, -1.56218641e-01, -3.70495135e-01,\n",
       "        -6.08972569e-01, -5.45126503e-01, -7.91206615e-01,\n",
       "        -2.51446403e-01, -2.58559662e-01, -3.85708419e-01,\n",
       "        -1.66229768e-06]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code inspiration taken from: \n",
    "# https://stackoverflow.com/questions/38787612/how-to-extract-feature-importances-from-an-sklearn-pipeline\n",
    "logreg_optimized_pipe.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_names_in_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9f9e43a3faa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# https://stackoverflow.com/questions/54646709/sklearn-pipeline-get-feature-names-after-onehotencode-in-columntransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogreg_optimized_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"log_reg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names_in_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#ohe_names = list(logreg_optimized_pipe.named_steps[\"preprocessing_pipe\"][2].transformers_[2][1].get_feature_names_out())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#ohe_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_names_in_'"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/54646709/sklearn-pipeline-get-feature-names-after-onehotencode-in-columntransformer\n",
    "logreg_optimized_pipe.named_steps[\"log_reg\"].feature_names_in_\n",
    "#ohe_names = list(logreg_optimized_pipe.named_steps[\"preprocessing_pipe\"][2].transformers_[2][1].get_feature_names_out())\n",
    "#ohe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.07761\n",
      "Feature: 1, Score: -0.04711\n",
      "Feature: 2, Score: 0.00172\n",
      "Feature: 3, Score: 0.14567\n",
      "Feature: 4, Score: -0.00253\n",
      "Feature: 5, Score: -0.04561\n",
      "Feature: 6, Score: 0.24970\n",
      "Feature: 7, Score: 1.29665\n",
      "Feature: 8, Score: 0.17907\n",
      "Feature: 9, Score: 0.03602\n",
      "Feature: 10, Score: 0.80946\n",
      "Feature: 11, Score: 0.43126\n",
      "Feature: 12, Score: 2.30595\n",
      "Feature: 13, Score: 2.15343\n",
      "Feature: 14, Score: -0.89763\n",
      "Feature: 15, Score: -0.20012\n",
      "Feature: 16, Score: -0.15142\n",
      "Feature: 17, Score: -0.87220\n",
      "Feature: 18, Score: -0.68040\n",
      "Feature: 19, Score: -0.48929\n",
      "Feature: 20, Score: -0.21722\n",
      "Feature: 21, Score: 0.57898\n",
      "Feature: 22, Score: -0.30980\n",
      "Feature: 23, Score: -0.48035\n",
      "Feature: 24, Score: -0.05867\n",
      "Feature: 25, Score: -0.24567\n",
      "Feature: 26, Score: -0.66476\n",
      "Feature: 27, Score: -0.47249\n",
      "Feature: 28, Score: -0.29140\n",
      "Feature: 29, Score: -0.32768\n",
      "Feature: 30, Score: -0.42261\n",
      "Feature: 31, Score: -0.40911\n",
      "Feature: 32, Score: -0.35700\n",
      "Feature: 33, Score: -0.23070\n",
      "Feature: 34, Score: -0.49699\n",
      "Feature: 35, Score: -0.32706\n",
      "Feature: 36, Score: -0.46874\n",
      "Feature: 37, Score: -0.15622\n",
      "Feature: 38, Score: -0.37050\n",
      "Feature: 39, Score: -0.60897\n",
      "Feature: 40, Score: -0.54513\n",
      "Feature: 41, Score: -0.79121\n",
      "Feature: 42, Score: -0.25145\n",
      "Feature: 43, Score: -0.25856\n",
      "Feature: 44, Score: -0.38571\n",
      "Feature: 45, Score: -0.00000\n"
     ]
    }
   ],
   "source": [
    "coefficients = logreg_optimized_pipe.steps[1][1].coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(coefficients):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df = pd.concat([pd.DataFrame(X_train.columns), pd.DataFrame(np.transpose(coefficients))], axis = 1)\n",
    "coefficients_df.columns = [\"feature_names\", \"coefficients\"]\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51328373/pandas-replace-nan-values-with-a-list-for-specific-columns\n",
    "coefficients_df.loc[coefficients_df[\"feature_names\"].isnull(), \"feature_names\"] = ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df['feature_names'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
